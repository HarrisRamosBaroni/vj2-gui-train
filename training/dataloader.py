# vj2_dataloader.py - Loads pre-processed (State, Action) pairs
import numpy as np
from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
from logging import getLogger
import os

logger = getLogger(__name__)

# ---------------------------------------------------------------------------

class PreprocessedGUIAgentDataset(Dataset):
    """
    A PyTorch Dataset for loading pre-processed trajectories, as generated by
    `process_raw_data.py`.

    The dataset expects files in .npz format, each containing 'embeddings'
    and 'actions' arrays. It assumes that each file represents a continuous
    recording session, and each item in the arrays is a full training
    trajectory (e.g., 16 embeddings and 7 action blocks).

    Returns a single trajectory of (embeddings, actions).

    Parameters
    ----------
    processed_data_dir: str
        Directory containing the pre-processed .npz files.
    """

    def __init__(self, processed_data_dir: str):
        self.data_dir = Path(processed_data_dir)
        self.file_indices = []
        self.total_trajectories = 0
        self._cached_data = {}

        data_files = sorted(self.data_dir.glob("*.npz"))
        if not data_files:
            raise RuntimeError(f"No .npz files found in {self.data_dir}. "
                               f"Did you run process_raw_data.py first?")

        logger.info(f"Indexing data from {len(data_files)} files...")
        for file_path in data_files:
            try:
                with np.load(file_path) as data:
                    if 'embeddings' in data and 'actions' in data:
                        num_trajectories = len(data['embeddings'])
                        if num_trajectories > 0:
                            self.file_indices.append({
                                'path': file_path,
                                'num_trajectories': num_trajectories,
                                'start_global_idx': self.total_trajectories
                            })
                            self.total_trajectories += num_trajectories
                        else:
                            logger.warning(f"Skipping {file_path}: contains 0 trajectories.")
                    else:
                        logger.warning(f"Skipping {file_path}: missing 'embeddings' or 'actions' key.")
            except Exception as e:
                logger.error(f"Could not load or process file {file_path} for indexing: {e}")

        if self.total_trajectories == 0:
            raise RuntimeError("No valid trajectories found across all .npz files. Dataset is empty.")

        logger.info(f"Successfully indexed {self.total_trajectories} trajectories.")


    def __len__(self):
        return self.total_trajectories

    def __getitem__(self, idx: int):
        for file_info in self.file_indices:
            if idx < file_info['start_global_idx'] + file_info['num_trajectories']:
                file_path = file_info['path']
                local_idx = idx - file_info['start_global_idx']
                break
        else:
            raise IndexError(f"Index {idx} out of bounds for dataset with {self.total_trajectories} trajectories.")

        # In-memory array caching: To avoid repeated disk I/O and decompression,
        # the entire 'embeddings' and 'actions' arrays are loaded into memory
        # the first time a file is accessed by a worker.
        file_path_str = str(file_path)
        if file_path_str not in self._cached_data:
            with np.load(file_path) as data:
                # Load the entire arrays into the cache
                self._cached_data[file_path_str] = (data['embeddings'], data['actions'])

        # Retrieve arrays from cache and slice the specific trajectory
        embeddings_array, actions_array = self._cached_data[file_path_str]
        embeddings = embeddings_array[local_idx]
        actions = actions_array[local_idx]

        return torch.from_numpy(embeddings).float(), torch.from_numpy(actions).float()

# ---------------------------------------------------------------------------

def init_preprocessed_data_loader(processed_data_dir: str, batch_size: int, num_workers=0):
    """Initializes the DataLoader for the pre-processed dataset."""
    dataset = PreprocessedGUIAgentDataset(
        processed_data_dir=processed_data_dir
    )
    
    # Check for distributed training environment
    use_sampler = torch.distributed.is_available() and torch.distributed.is_initialized()
    
    sampler = None
    if use_sampler:
        sampler = torch.utils.data.distributed.DistributedSampler(dataset, shuffle=True)
    
    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(sampler is None), # Shuffle only if not using a distributed sampler
        num_workers=num_workers,
        pin_memory=True,
        sampler=sampler
    )
    
    # DDP sanity prints
    if torch.distributed.is_available() and torch.distributed.is_initialized():
        rank = torch.distributed.get_rank()
        world_size = torch.distributed.get_world_size()
        if rank == 0:
            logger.info(f"[DDP] Dataloader created with {len(dataset)} samples, batch_size={batch_size}")
            if sampler:
                logger.info(f"[DDP] Using DistributedSampler with {world_size} ranks")
    else:
        logger.info("Preprocessed GUI Agent data loader created.")
    return data_loader, sampler

# ---------------------------------------------------------------------------

if __name__ == "__main__":
    import logging
    # This block is for testing the data loader.
    # It assumes you have already run `process_raw_data.py` and have data
    # in the './processed_data' directory.
    
    logging.basicConfig(level=logging.INFO)
    logger.info("Testing PreprocessedGUIAgentDataset...")

    processed_dir = "./processed_data"
    
    if not Path(processed_dir).exists() or not any(Path(processed_dir).iterdir()):
         logger.warning(f"'{processed_dir}' is empty or doesn't exist.")
         logger.warning("Please run process_raw_data.py to generate test data.")
    else:
        try:
            test_loader, _ = init_preprocessed_data_loader(
                processed_data_dir=processed_dir,
                batch_size=4
            )

            # Fetch one batch
            sample_embeddings, sample_actions = next(iter(test_loader))

            print("\n--- DataLoader Test ---")
            print(f"Batch embeddings shape: {sample_embeddings.shape}")
            print(f"Batch actions shape:      {sample_actions.shape}")
            print("--- Test Successful ---")
            
        except Exception as e:
            print(f"\n--- Test Failed ---")
            print(f"An error occurred during the test: {e}")